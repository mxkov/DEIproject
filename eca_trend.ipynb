{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP = \"192.168.1.1\"  # rember to set the real IP!\n",
    "cores = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/01 10:22:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder\\\n",
    "        .master(f\"spark://{IP}:7077\")\\\n",
    "        .appName(\"ECAtemp\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", cores)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark_session.sparkContext\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data at a glance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data should have been extracted to HDFS: ```/data/tx/``` and ```/data/tn/``` for maximum and minimum temperatures, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the contents of the directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2064 items\n",
      "-rw-r--r--   1 ubuntu supergroup    1637898 2024-08-01 10:01 /data/tx/TX_STAID000002.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874571 2024-08-01 10:01 /data/tx/TX_STAID000004.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874582 2024-08-01 10:01 /data/tx/TX_STAID000005.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1080231 2024-08-01 10:01 /data/tx/TX_STAID000007.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874586 2024-08-01 10:01 /data/tx/TX_STAID000008.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874592 2024-08-01 10:01 /data/tx/TX_STAID000009.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874571 2024-08-01 10:01 /data/tx/TX_STAID000010.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1979781 2024-08-01 10:01 /data/tx/TX_STAID000011.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1716813 2024-08-01 10:01 /data/tx/TX_STAID000012.txt\n",
      "\n",
      "Found 2064 items\n",
      "-rw-r--r--   1 ubuntu supergroup    1637896 2024-08-01 09:56 /data/tn/TN_STAID000002.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874571 2024-08-01 09:56 /data/tn/TN_STAID000004.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874582 2024-08-01 09:56 /data/tn/TN_STAID000005.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1080231 2024-08-01 09:56 /data/tn/TN_STAID000007.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874586 2024-08-01 09:56 /data/tn/TN_STAID000008.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874592 2024-08-01 09:56 /data/tn/TN_STAID000009.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874571 2024-08-01 09:56 /data/tn/TN_STAID000010.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1979781 2024-08-01 09:56 /data/tn/TN_STAID000011.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1716820 2024-08-01 09:56 /data/tn/TN_STAID000012.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data_id in (\"tx\", \"tn\"):\n",
    "    p = subprocess.run([\"/home/ubuntu/hadoop-3.3.6/bin/hdfs\",\n",
    "                        \"dfs\", \"-ls\", f\"/data/{data_id}\"],\n",
    "                       check=True, capture_output=True)\n",
    "    ls_head = subprocess.run(\"head\", input=p.stdout, capture_output=True)\n",
    "    print(ls_head.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a preview of the first file in ```/data/tx/```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUROPEAN CLIMATE ASSESSMENT & DATASET (ECA&D), file created on: 13-07-2024\n",
      "THESE DATA CAN BE USED FOR NON-COMMERCIAL RESEARCH AND EDUCATION PROVIDED THAT THE FOLLOWING SOURCE IS ACKNOWLEDGED: \n",
      "\n",
      "Klein Tank, A.M.G. and Coauthors, 2002. Daily dataset of 20th-century surface\n",
      "air temperature and precipitation series for the European Climate Assessment.\n",
      "Int. J. of Climatol., 22, 1441-1453.\n",
      "Data and metadata available at http://www.ecad.eu\n",
      "\n",
      "FILE FORMAT (MISSING VALUE CODE = -9999):\n",
      "\n",
      "01-06 STAID: Station identifier\n",
      "08-13 SOUID: Source identifier\n",
      "15-22 DATE : Date YYYYMMDD\n",
      "24-28 TX   : Maximum temperature in 0.1 &#176;C\n",
      "30-34 Q_TX : quality code for TX (0='valid'; 1='suspect'; 9='missing')\n",
      "\n",
      "This is the blended series of station FALUN, SWEDEN (STAID: 2)\n",
      "Blended and updated with sources:7 36438 \n",
      "See files sources.txt and stations.txt for more info.\n",
      "\n",
      "STAID, SOUID,    DATE,   TX, Q_TX\n",
      "     2, 36438,19000101,   18,    0\n",
      "     2, 36438,19000102,  -10,    0\n",
      "     2, 36438,19000103,  -20,    0\n",
      "     2, 36438,19000104,  -30,    0\n",
      "     2, 36438,19000105,  -60,    0\n",
      "     2, 36438,19000106, -150,    0\n",
      "     2, 36438,19000107,  -90,    0\n",
      "     2, 36438,19000108,  -40,    0\n",
      "     2, 36438,19000109,  -40,    0\n"
     ]
    }
   ],
   "source": [
    "ls = subprocess.Popen([\"/home/ubuntu/hadoop-3.3.6/bin/hdfs\",\n",
    "                       \"dfs\", \"-ls\", \"/data/tx\"],\n",
    "                      stdout=subprocess.PIPE)\n",
    "sample_file = ls.communicate()[0].decode().split(\"\\n\")[1].strip().split()[-1]\n",
    "\n",
    "cat = subprocess.Popen([\"/home/ubuntu/hadoop-3.3.6/bin/hdfs\",\n",
    "                        \"dfs\", \"-cat\", sample_file],\n",
    "                       stdout=subprocess.PIPE)\n",
    "preview = \"\\n\".join(cat.communicate()[0].decode().split(\"\\n\")[:30])\n",
    "print(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = f\"hdfs://{IP}:9000/data/\"\n",
    "rdds = {}\n",
    "\n",
    "for data_id in (\"tx\", \"tn\"):\n",
    "    data_dir = data_root + data_id\n",
    "    rdds[data_id] = sc.textFile(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the max and min temperature data by exluding the first 20 lines from each file and converting the rest into a proper dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(rdd):\n",
    "\n",
    "    # read every file excluding first 20 lines\n",
    "    rdd = rdd\\\n",
    "        .zipWithIndex()\\\n",
    "        .filter(lambda x: x[1] >= 20)\\\n",
    "        .map(lambda x: (x[0], ))\\\n",
    "        .toDF()\n",
    "\n",
    "    # get column names from 1st row\n",
    "    column_names = rdd.first()[\"_1\"].split(\",\")\n",
    "    column_names = [x.strip() for x in column_names]\n",
    "    # drop 1st row\n",
    "    rdd = rdd.withColumn(\"index\", monotonically_increasing_id())\n",
    "    rdd = rdd\\\n",
    "        .filter(rdd.index > 0)\\\n",
    "        .drop(\"index\")\n",
    "\n",
    "    # form columns\n",
    "    for i,name in enumerate(column_names):\n",
    "        rdd = rdd.withColumn(name, split(rdd[\"_1\"], \",\").getItem(i))\n",
    "        rdd = rdd.withColumn(name, rdd[name].cast(\"int\"))\n",
    "    rdd = rdd.drop(\"_1\")\n",
    "\n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+----+----+\n",
      "|STAID|SOUID|    DATE|  TX|Q_TX|\n",
      "+-----+-----+--------+----+----+\n",
      "|    2|36438|19000101|  18|   0|\n",
      "|    2|36438|19000102| -10|   0|\n",
      "|    2|36438|19000103| -20|   0|\n",
      "|    2|36438|19000104| -30|   0|\n",
      "|    2|36438|19000105| -60|   0|\n",
      "|    2|36438|19000106|-150|   0|\n",
      "|    2|36438|19000107| -90|   0|\n",
      "|    2|36438|19000108| -40|   0|\n",
      "|    2|36438|19000109| -40|   0|\n",
      "|    2|36438|19000110| -10|   0|\n",
      "|    2|36438|19000111| -24|   0|\n",
      "|    2|36438|19000112|-120|   0|\n",
      "|    2|36438|19000113|-100|   0|\n",
      "|    2|36438|19000114| -60|   0|\n",
      "|    2|36438|19000115| -70|   0|\n",
      "|    2|36438|19000116| -10|   0|\n",
      "|    2|36438|19000117| -20|   0|\n",
      "|    2|36438|19000118| -20|   0|\n",
      "|    2|36438|19000119| -34|   0|\n",
      "|    2|36438|19000120| -20|   0|\n",
      "+-----+-----+--------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+----+----+\n",
      "|STAID|SOUID|    DATE|  TN|Q_TN|\n",
      "+-----+-----+--------+----+----+\n",
      "|    2|36439|19000101| -30|   0|\n",
      "|    2|36439|19000102| -50|   0|\n",
      "|    2|36439|19000103| -70|   0|\n",
      "|    2|36439|19000104|-110|   0|\n",
      "|    2|36439|19000105|-190|   0|\n",
      "|    2|36439|19000106|-220|   0|\n",
      "|    2|36439|19000107|-200|   0|\n",
      "|    2|36439|19000108|-110|   0|\n",
      "|    2|36439|19000109| -80|   0|\n",
      "|    2|36439|19000110| -60|   0|\n",
      "|    2|36439|19000111|-160|   0|\n",
      "|    2|36439|19000112|-210|   0|\n",
      "|    2|36439|19000113|-220|   0|\n",
      "|    2|36439|19000114|-130|   0|\n",
      "|    2|36439|19000115|-190|   0|\n",
      "|    2|36439|19000116|-110|   0|\n",
      "|    2|36439|19000117| -60|   0|\n",
      "|    2|36439|19000118| -60|   0|\n",
      "|    2|36439|19000119|-130|   0|\n",
      "|    2|36439|19000120|-120|   0|\n",
      "+-----+-----+--------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data_id in (\"tx\", \"tn\"):\n",
    "    rdds[data_id] = Preprocess(rdds[data_id])\n",
    "    rdds[data_id].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
