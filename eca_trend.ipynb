{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFS_EXE = \"/home/ubuntu/hadoop-3.3.6/bin/hdfs\"\n",
    "IP = \"192.168.1.1\"  # rember to set the real IP!\n",
    "cores = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/01 15:35:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder\\\n",
    "        .master(f\"spark://{IP}:7077\")\\\n",
    "        .appName(\"ECAtemp\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", cores)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark_session.sparkContext\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data at a glance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data should have been extracted to HDFS: ```/data/tx/``` and ```/data/tn/``` for maximum and minimum temperatures, respectively. In addition, there is a station metadata file: ```/data/station_data.txt```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the contents of the directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2064 items\n",
      "-rw-r--r--   1 ubuntu supergroup    1637898 2024-08-01 10:01 /data/tx/TX_STAID000002.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874571 2024-08-01 10:01 /data/tx/TX_STAID000004.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874582 2024-08-01 10:01 /data/tx/TX_STAID000005.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1080231 2024-08-01 10:01 /data/tx/TX_STAID000007.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874586 2024-08-01 10:01 /data/tx/TX_STAID000008.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874592 2024-08-01 10:01 /data/tx/TX_STAID000009.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874571 2024-08-01 10:01 /data/tx/TX_STAID000010.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1979781 2024-08-01 10:01 /data/tx/TX_STAID000011.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1716813 2024-08-01 10:01 /data/tx/TX_STAID000012.txt\n",
      "\n",
      "Found 2064 items\n",
      "-rw-r--r--   1 ubuntu supergroup    1637896 2024-08-01 09:56 /data/tn/TN_STAID000002.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874571 2024-08-01 09:56 /data/tn/TN_STAID000004.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874582 2024-08-01 09:56 /data/tn/TN_STAID000005.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1080231 2024-08-01 09:56 /data/tn/TN_STAID000007.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874586 2024-08-01 09:56 /data/tn/TN_STAID000008.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874592 2024-08-01 09:56 /data/tn/TN_STAID000009.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1874571 2024-08-01 09:56 /data/tn/TN_STAID000010.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1979781 2024-08-01 09:56 /data/tn/TN_STAID000011.txt\n",
      "-rw-r--r--   1 ubuntu supergroup    1716820 2024-08-01 09:56 /data/tn/TN_STAID000012.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data_id in (\"tx\", \"tn\"):\n",
    "    p = subprocess.run([HDFS_EXE, \"dfs\", \"-ls\", f\"/data/{data_id}\"],\n",
    "                       check=True, capture_output=True)\n",
    "    p = subprocess.run(\"head\", input=p.stdout, capture_output=True)\n",
    "    print(p.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a preview of the first file in ```/data/tx/```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUROPEAN CLIMATE ASSESSMENT & DATASET (ECA&D), file created on: 13-07-2024\n",
      "THESE DATA CAN BE USED FOR NON-COMMERCIAL RESEARCH AND EDUCATION PROVIDED THAT THE FOLLOWING SOURCE IS ACKNOWLEDGED: \n",
      "\n",
      "Klein Tank, A.M.G. and Coauthors, 2002. Daily dataset of 20th-century surface\n",
      "air temperature and precipitation series for the European Climate Assessment.\n",
      "Int. J. of Climatol., 22, 1441-1453.\n",
      "Data and metadata available at http://www.ecad.eu\n",
      "\n",
      "FILE FORMAT (MISSING VALUE CODE = -9999):\n",
      "\n",
      "01-06 STAID: Station identifier\n",
      "08-13 SOUID: Source identifier\n",
      "15-22 DATE : Date YYYYMMDD\n",
      "24-28 TX   : Maximum temperature in 0.1 &#176;C\n",
      "30-34 Q_TX : quality code for TX (0='valid'; 1='suspect'; 9='missing')\n",
      "\n",
      "This is the blended series of station FALUN, SWEDEN (STAID: 2)\n",
      "Blended and updated with sources:7 36438 \n",
      "See files sources.txt and stations.txt for more info.\n",
      "\n",
      "STAID, SOUID,    DATE,   TX, Q_TX\n",
      "     2, 36438,19000101,   18,    0\n",
      "     2, 36438,19000102,  -10,    0\n",
      "     2, 36438,19000103,  -20,    0\n",
      "     2, 36438,19000104,  -30,    0\n",
      "     2, 36438,19000105,  -60,    0\n",
      "     2, 36438,19000106, -150,    0\n",
      "     2, 36438,19000107,  -90,    0\n",
      "     2, 36438,19000108,  -40,    0\n",
      "     2, 36438,19000109,  -40,    0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = subprocess.run([HDFS_EXE, \"dfs\", \"-ls\", \"/data/tx\"],\n",
    "                   check=True, capture_output=True)\n",
    "p = subprocess.run(\"head\", input=p.stdout, capture_output=True)\n",
    "sample_file = p.stdout.decode().split(\"\\n\")[1].strip().split()[-1]\n",
    "\n",
    "p = subprocess.run([HDFS_EXE, \"dfs\", \"-cat\", sample_file],\n",
    "                   check=True, capture_output=True)\n",
    "p = subprocess.run([\"head\", \"-30\"], input=p.stdout, capture_output=True)\n",
    "preview = p.stdout.decode()\n",
    "print(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a preview of the metadata file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAID,STANAME,CN,LAT,LON,HGHT\n",
      "2,FALUN,SE,60.616667,15.616667,160\n",
      "4,LINKOEPING,SE,58.4,15.533056,93\n",
      "5,LINKOEPING-MALMSLAETT,SE,58.4,15.533056,93\n",
      "7,KARLSTAD-AIRPORT,SE,59.444444,13.3375,107\n",
      "8,OESTERSUND,SE,63.183056,14.483056,376\n",
      "9,OESTERSUND-FROESOEN,SE,63.183056,14.483056,376\n",
      "10,STOCKHOLM,SE,59.35,18.05,44\n",
      "11,KREMSMUENSTER (TAWES),AT,48.055,14.130833,382\n",
      "12,GRAZ-UNIVERSITAET,AT,47.077778,15.448889,367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = subprocess.run([HDFS_EXE, \"dfs\", \"-cat\", \"/data/station_data.txt\"],\n",
    "                   check=True, capture_output=True)\n",
    "p = subprocess.run(\"head\", input=p.stdout, capture_output=True)\n",
    "preview_meta = p.stdout.decode()\n",
    "print(preview_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = f\"hdfs://{IP}:9000/data/\"\n",
    "rdds = {}\n",
    "dfs  = {}\n",
    "\n",
    "for data_id in (\"tx\", \"tn\"):\n",
    "    data_dir = data_root + data_id\n",
    "    rdds[data_id] = sc.textFile(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the max and min temperature data by exluding the first 20 lines from each file and converting the rest into a proper dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess_step1(rdd):\n",
    "\n",
    "    # read every file excluding first 20 lines\n",
    "    df = rdd\\\n",
    "        .zipWithIndex()\\\n",
    "        .filter(lambda x: x[1] >= 20)\\\n",
    "        .map(lambda x: (x[0], ))\\\n",
    "        .toDF()\n",
    "\n",
    "    # get column names from 1st row\n",
    "    column_names = df.first()[\"_1\"].split(\",\")\n",
    "    column_names = [x.strip() for x in column_names]\n",
    "    # drop 1st row\n",
    "    df = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "    df = df\\\n",
    "        .filter(df.index > 0)\\\n",
    "        .drop(\"index\")\n",
    "\n",
    "    # form columns\n",
    "    for i,name in enumerate(column_names):\n",
    "        df = df.withColumn(name, split(df[\"_1\"], \",\").getItem(i))\n",
    "        df = df.withColumn(name, df[name].cast(\"int\"))\n",
    "    df = df.drop(\"_1\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+----+----+\n",
      "|STAID|SOUID|    DATE|  TX|Q_TX|\n",
      "+-----+-----+--------+----+----+\n",
      "|    2|36438|19000101|  18|   0|\n",
      "|    2|36438|19000102| -10|   0|\n",
      "|    2|36438|19000103| -20|   0|\n",
      "|    2|36438|19000104| -30|   0|\n",
      "|    2|36438|19000105| -60|   0|\n",
      "|    2|36438|19000106|-150|   0|\n",
      "|    2|36438|19000107| -90|   0|\n",
      "|    2|36438|19000108| -40|   0|\n",
      "|    2|36438|19000109| -40|   0|\n",
      "|    2|36438|19000110| -10|   0|\n",
      "|    2|36438|19000111| -24|   0|\n",
      "|    2|36438|19000112|-120|   0|\n",
      "|    2|36438|19000113|-100|   0|\n",
      "|    2|36438|19000114| -60|   0|\n",
      "|    2|36438|19000115| -70|   0|\n",
      "|    2|36438|19000116| -10|   0|\n",
      "|    2|36438|19000117| -20|   0|\n",
      "|    2|36438|19000118| -20|   0|\n",
      "|    2|36438|19000119| -34|   0|\n",
      "|    2|36438|19000120| -20|   0|\n",
      "+-----+-----+--------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+----+----+\n",
      "|STAID|SOUID|    DATE|  TN|Q_TN|\n",
      "+-----+-----+--------+----+----+\n",
      "|    2|36439|19000101| -30|   0|\n",
      "|    2|36439|19000102| -50|   0|\n",
      "|    2|36439|19000103| -70|   0|\n",
      "|    2|36439|19000104|-110|   0|\n",
      "|    2|36439|19000105|-190|   0|\n",
      "|    2|36439|19000106|-220|   0|\n",
      "|    2|36439|19000107|-200|   0|\n",
      "|    2|36439|19000108|-110|   0|\n",
      "|    2|36439|19000109| -80|   0|\n",
      "|    2|36439|19000110| -60|   0|\n",
      "|    2|36439|19000111|-160|   0|\n",
      "|    2|36439|19000112|-210|   0|\n",
      "|    2|36439|19000113|-220|   0|\n",
      "|    2|36439|19000114|-130|   0|\n",
      "|    2|36439|19000115|-190|   0|\n",
      "|    2|36439|19000116|-110|   0|\n",
      "|    2|36439|19000117| -60|   0|\n",
      "|    2|36439|19000118| -60|   0|\n",
      "|    2|36439|19000119|-130|   0|\n",
      "|    2|36439|19000120|-120|   0|\n",
      "+-----+-----+--------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data_id in (\"tx\", \"tn\"):\n",
    "    dfs[data_id] = Preprocess_step1(rdds[data_id])\n",
    "    dfs[data_id].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to convert the date from string to an appropriate type and place NAs where missing values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess_step2(df, data_id):\n",
    "    temper_col = data_id.upper()\n",
    "    df = df.withColumn(\"DATE\", to_date(df[\"DATE\"], format=\"yyyyMMdd\"))\n",
    "    df = df.withColumn(temper_col, when(df[f\"Q_{temper_col}\"]==0,\n",
    "                                        df[temper_col]).otherwise(None))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+----+----+\n",
      "|STAID|SOUID|      DATE|  TX|Q_TX|\n",
      "+-----+-----+----------+----+----+\n",
      "|    2|36438|1900-01-01|  18|   0|\n",
      "|    2|36438|1900-01-02| -10|   0|\n",
      "|    2|36438|1900-01-03| -20|   0|\n",
      "|    2|36438|1900-01-04| -30|   0|\n",
      "|    2|36438|1900-01-05| -60|   0|\n",
      "|    2|36438|1900-01-06|-150|   0|\n",
      "|    2|36438|1900-01-07| -90|   0|\n",
      "|    2|36438|1900-01-08| -40|   0|\n",
      "|    2|36438|1900-01-09| -40|   0|\n",
      "|    2|36438|1900-01-10| -10|   0|\n",
      "|    2|36438|1900-01-11| -24|   0|\n",
      "|    2|36438|1900-01-12|-120|   0|\n",
      "|    2|36438|1900-01-13|-100|   0|\n",
      "|    2|36438|1900-01-14| -60|   0|\n",
      "|    2|36438|1900-01-15| -70|   0|\n",
      "|    2|36438|1900-01-16| -10|   0|\n",
      "|    2|36438|1900-01-17| -20|   0|\n",
      "|    2|36438|1900-01-18| -20|   0|\n",
      "|    2|36438|1900-01-19| -34|   0|\n",
      "|    2|36438|1900-01-20| -20|   0|\n",
      "+-----+-----+----------+----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- STAID: integer (nullable = true)\n",
      " |-- SOUID: integer (nullable = true)\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- TX: integer (nullable = true)\n",
      " |-- Q_TX: integer (nullable = true)\n",
      "\n",
      "+-----+-----+----------+----+----+\n",
      "|STAID|SOUID|      DATE|  TN|Q_TN|\n",
      "+-----+-----+----------+----+----+\n",
      "|    2|36439|1900-01-01| -30|   0|\n",
      "|    2|36439|1900-01-02| -50|   0|\n",
      "|    2|36439|1900-01-03| -70|   0|\n",
      "|    2|36439|1900-01-04|-110|   0|\n",
      "|    2|36439|1900-01-05|-190|   0|\n",
      "|    2|36439|1900-01-06|-220|   0|\n",
      "|    2|36439|1900-01-07|-200|   0|\n",
      "|    2|36439|1900-01-08|-110|   0|\n",
      "|    2|36439|1900-01-09| -80|   0|\n",
      "|    2|36439|1900-01-10| -60|   0|\n",
      "|    2|36439|1900-01-11|-160|   0|\n",
      "|    2|36439|1900-01-12|-210|   0|\n",
      "|    2|36439|1900-01-13|-220|   0|\n",
      "|    2|36439|1900-01-14|-130|   0|\n",
      "|    2|36439|1900-01-15|-190|   0|\n",
      "|    2|36439|1900-01-16|-110|   0|\n",
      "|    2|36439|1900-01-17| -60|   0|\n",
      "|    2|36439|1900-01-18| -60|   0|\n",
      "|    2|36439|1900-01-19|-130|   0|\n",
      "|    2|36439|1900-01-20|-120|   0|\n",
      "+-----+-----+----------+----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- STAID: integer (nullable = true)\n",
      " |-- SOUID: integer (nullable = true)\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- TN: integer (nullable = true)\n",
      " |-- Q_TN: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data_id in (\"tx\", \"tn\"):\n",
    "    dfs[data_id] = Preprocess_step2(dfs[data_id], data_id)\n",
    "    dfs[data_id].show()\n",
    "    dfs[data_id].printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to read the station metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+---------+---------+----+\n",
      "|STAID|             STANAME| CN|      LAT|      LON|HGHT|\n",
      "+-----+--------------------+---+---------+---------+----+\n",
      "|    2|               FALUN| SE| 60.61667|15.616667| 160|\n",
      "|    4|          LINKOEPING| SE|     58.4|15.533056|  93|\n",
      "|    5|LINKOEPING-MALMSL...| SE|     58.4|15.533056|  93|\n",
      "|    7|    KARLSTAD-AIRPORT| SE|59.444443|  13.3375| 107|\n",
      "|    8|          OESTERSUND| SE|63.183056|14.483056| 376|\n",
      "|    9| OESTERSUND-FROESOEN| SE|63.183056|14.483056| 376|\n",
      "|   10|           STOCKHOLM| SE|    59.35|    18.05|  44|\n",
      "|   11|KREMSMUENSTER (TA...| AT|   48.055|14.130833| 382|\n",
      "|   12|   GRAZ-UNIVERSITAET| AT|47.077778|15.448889| 367|\n",
      "|   13|      INNSBRUCK-UNIV| AT|    47.26|11.384167| 578|\n",
      "|   14|  SALZBURG-FLUGHAFEN| AT|47.789165|13.008056| 430|\n",
      "|   15|   SONNBLICK (TAWES)| AT| 47.05417|  12.9575|3109|\n",
      "|   16|     WIEN-HOHE WARTE| AT|48.248333|16.356388| 198|\n",
      "|   21|         ZAGREB-GRIC| HR|45.816666|15.978056| 156|\n",
      "|   28| HELSINKI KAISANIEMI| FI|   60.175|24.947779|   4|\n",
      "|   29|JYVASKYLA LENTOASEMA| FI|62.401943|25.678333| 139|\n",
      "|   30|SODANKYLA. LAPIN ...| FI| 67.36583|26.633057| 179|\n",
      "|   32|             BOURGES| FR|47.059166| 2.359444| 161|\n",
      "|   33|    TOULOUSE-BLAGNAC| FR|43.620834| 1.378889| 151|\n",
      "|   34|   BORDEAUX-MERIGNAC| FR|44.830555| 0.691389|  47|\n",
      "+-----+--------------------+---+---------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- STAID: string (nullable = true)\n",
      " |-- STANAME: string (nullable = true)\n",
      " |-- CN: string (nullable = true)\n",
      " |-- LAT: float (nullable = true)\n",
      " |-- LON: float (nullable = true)\n",
      " |-- HGHT: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta = spark_session.read.csv(data_root + \"station_data.txt\", header=True)\n",
    "for col in (\"LAT\", \"LON\"):\n",
    "    meta = meta.withColumn(col, meta[col].cast(\"float\"))\n",
    "meta = meta.withColumn(\"HGHT\", meta[\"HGHT\"].cast(\"int\"))\n",
    "meta.show()\n",
    "meta.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST CELLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|    a|  b|\n",
      "+-----+---+\n",
      "|   18|  0|\n",
      "|-9999|  9|\n",
      "|   45|  0|\n",
      "|  -56|  1|\n",
      "|-9999|  9|\n",
      "+-----+---+\n",
      "\n",
      "+----+---+\n",
      "|   a|  b|\n",
      "+----+---+\n",
      "|  18|  0|\n",
      "|NULL|  9|\n",
      "|  45|  0|\n",
      "|NULL|  1|\n",
      "|NULL|  9|\n",
      "+----+---+\n",
      "\n",
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "| 18|  0|\n",
      "| 45|  0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tst = spark_session.createDataFrame([(18, 0), (-9999, 9), (45, 0), (-56, 1), (-9999, 9)], (\"a\", \"b\"))\n",
    "tst.show()\n",
    "tst = tst.withColumn(\"a\", when(tst[\"b\"]==0, tst[\"a\"]).otherwise(None))\n",
    "tst.show()\n",
    "tst.dropna(\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
