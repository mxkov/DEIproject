{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP = \"192.168.1.1\"  # rember to set the real IP!\n",
    "cores = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/31 21:23:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder\\\n",
    "        .master(f\"spark://{IP}:7077\")\\\n",
    "        .appName(\"ECAtemp\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", cores)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark_session.sparkContext\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data should have been extracted to HDFS. Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = f\"hdfs://{IP}:9000/data/\"\n",
    "rdds = {}\n",
    "\n",
    "for data_id in (\"tx\", \"tn\"):\n",
    "    data_dir = data_root + data_id\n",
    "    rdds[data_id] = sc.textFile(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(rdd):\n",
    "\n",
    "    # read every file excluding first 20 lines\n",
    "    rdd = rdd\\\n",
    "        .zipWithIndex()\\\n",
    "        .filter(lambda x: x[1] >= 20)\\\n",
    "        .map(lambda x: (x[0], ))\\\n",
    "        .toDF()\n",
    "\n",
    "    # get column names from 1st row\n",
    "    column_names = rdd.first()[\"_1\"].split(\",\")\n",
    "    column_names = [x.strip() for x in column_names]\n",
    "    # drop 1st row\n",
    "    rdd = rdd.withColumn(\"index\", monotonically_increasing_id())\n",
    "    rdd = rdd\\\n",
    "        .filter(rdd.index > 0)\\\n",
    "        .drop(\"index\")\n",
    "\n",
    "    # form columns\n",
    "    for i,name in enumerate(column_names):\n",
    "        rdd = rdd.withColumn(name, split(rdd[\"_1\"], \",\").getItem(i))\n",
    "        rdd = rdd.withColumn(name, rdd[name].cast(\"int\"))\n",
    "    rdd = rdd.drop(\"_1\")\n",
    "\n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+----+----+\n",
      "|STAID|SOUID|    DATE|  TX|Q_TX|\n",
      "+-----+-----+--------+----+----+\n",
      "|    2|36438|19000101|  18|   0|\n",
      "|    2|36438|19000102| -10|   0|\n",
      "|    2|36438|19000103| -20|   0|\n",
      "|    2|36438|19000104| -30|   0|\n",
      "|    2|36438|19000105| -60|   0|\n",
      "|    2|36438|19000106|-150|   0|\n",
      "|    2|36438|19000107| -90|   0|\n",
      "|    2|36438|19000108| -40|   0|\n",
      "|    2|36438|19000109| -40|   0|\n",
      "|    2|36438|19000110| -10|   0|\n",
      "|    2|36438|19000111| -24|   0|\n",
      "|    2|36438|19000112|-120|   0|\n",
      "|    2|36438|19000113|-100|   0|\n",
      "|    2|36438|19000114| -60|   0|\n",
      "|    2|36438|19000115| -70|   0|\n",
      "|    2|36438|19000116| -10|   0|\n",
      "|    2|36438|19000117| -20|   0|\n",
      "|    2|36438|19000118| -20|   0|\n",
      "|    2|36438|19000119| -34|   0|\n",
      "|    2|36438|19000120| -20|   0|\n",
      "+-----+-----+--------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+----+----+\n",
      "|STAID|SOUID|    DATE|  TN|Q_TN|\n",
      "+-----+-----+--------+----+----+\n",
      "|    2|36439|19000101| -30|   0|\n",
      "|    2|36439|19000102| -50|   0|\n",
      "|    2|36439|19000103| -70|   0|\n",
      "|    2|36439|19000104|-110|   0|\n",
      "|    2|36439|19000105|-190|   0|\n",
      "|    2|36439|19000106|-220|   0|\n",
      "|    2|36439|19000107|-200|   0|\n",
      "|    2|36439|19000108|-110|   0|\n",
      "|    2|36439|19000109| -80|   0|\n",
      "|    2|36439|19000110| -60|   0|\n",
      "|    2|36439|19000111|-160|   0|\n",
      "|    2|36439|19000112|-210|   0|\n",
      "|    2|36439|19000113|-220|   0|\n",
      "|    2|36439|19000114|-130|   0|\n",
      "|    2|36439|19000115|-190|   0|\n",
      "|    2|36439|19000116|-110|   0|\n",
      "|    2|36439|19000117| -60|   0|\n",
      "|    2|36439|19000118| -60|   0|\n",
      "|    2|36439|19000119|-130|   0|\n",
      "|    2|36439|19000120|-120|   0|\n",
      "+-----+-----+--------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data_id in (\"tx\", \"tn\"):\n",
    "    rdds[data_id] = Preprocess(rdds[data_id])\n",
    "    rdds[data_id].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
